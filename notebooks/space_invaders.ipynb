{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bac46d0-4abc-49a5-8aaa-7c58a2014647",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Replay Memory: written first version\n",
    "    - add batches of transitions\n",
    "    - changed _batch_random to return a tuple since tf.data.Dataset is for largedatasets\n",
    "    - check if the tensorarray can be raplaced with tf.data.Datasets\n",
    "- Preprocessing:\n",
    "    - transform the frames to tensorflow tensors asap and try to use tf.function where possible\n",
    "    - try to do the preprocessing in batches to use parallelism\n",
    "    - try to use lazy loading and prefetching as well\n",
    "- DQN Model\n",
    "    - Written a CNN model with custom loss function\n",
    "    - check if the compile is still neeeded\n",
    "    - check if I should write the class as a more general module and not subclass of Sequential\n",
    "- DQN Agent\n",
    "    - started writing\n",
    "    - update_step vectorized (and could also be decorated with tf.function)\n",
    "    - need to check if it works properly\n",
    "    - need to see if I want to make larger batch or more batches for one update_step\n",
    "    - Training loop\n",
    "        - needs to be checked what should be vectorized\n",
    "        - need to substitute python and numpy code with tensorflow code\n",
    "        - optimize for graph computations\n",
    "- Use a profilier to optimize code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c488bc11-a430-41e8-a3dc-4cb9f5e6e5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/harisc/repos/deep-q-learning'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "os.chdir('/home/harisc/repos/deep-q-learning')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa32ad4c-4ae0-4979-bff5-f25c6f45f873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-23 22:37:30.952077: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-23 22:37:30.996495: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-23 22:37:30.996527: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-23 22:37:30.997803: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-23 22:37:31.004429: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-23 22:37:31.004901: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-23 22:37:31.891846: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from src.agents.dqn_agent import SpaceInvaderAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f2b9290-a770-4944-8646-bce7e9245382",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "memory_size = 5000 # 10**4 # 3 * 10**5\n",
    "batch_size = 32 # 32\n",
    "max_train_frames = 5000 # 5 * 10**4 # 4.5 * 10**5\n",
    "update_main_freq = 4\n",
    "update_target_freq = 250 # 0.25 * 10**4 # 10**4\n",
    "average_loss_freq = 200 # 400\n",
    "memory_warmup = 200 # 0.5 * 10**4 # 5*10**4\n",
    "discount = 0.99\n",
    "\n",
    "eval_episodes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8db547f4-5159-4e21-9dbe-023198111e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_agent = SpaceInvaderAgent(\n",
    "    learning_rate = learning_rate,\n",
    "    memory_size = memory_size,\n",
    "    batch_size = batch_size,\n",
    "    max_train_frames = max_train_frames,\n",
    "    update_main_freq = update_main_freq,\n",
    "    update_target_freq = update_target_freq,\n",
    "    average_loss_freq = average_loss_freq,\n",
    "    memory_warmup = memory_warmup,\n",
    "    discount = discount,\n",
    "    eval_episodes = eval_episodes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89f649a5-1bbb-4f6a-be1e-bab0fca813cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fccec4d8850>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    return outflag  File \"/tmp/ipykernel_20874/2141352408.py\", line 2, in <module>\n",
      "    training_losses, training_rewards = my_agent.train()  File \"/home/harisc/repos/deep-q-learning/src/agents/dqn_agent.py\", line 134, in train\n",
      "    return self.averaged_losses, self.rewards  File \"/home/harisc/repos/deep-q-learning/src/utils/replay_memory.py\", line 27, in add_transition\n",
      "    self.idx += 1  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fccec4d8850>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    return outflag  File \"/tmp/ipykernel_20874/2141352408.py\", line 2, in <module>\n",
      "    training_losses, training_rewards = my_agent.train()  File \"/home/harisc/repos/deep-q-learning/src/agents/dqn_agent.py\", line 134, in train\n",
      "    return self.averaged_losses, self.rewards  File \"/home/harisc/repos/deep-q-learning/src/utils/replay_memory.py\", line 27, in add_transition\n",
      "    self.idx += 1  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fccec5125d0>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    return outflag  File \"/tmp/ipykernel_20874/2141352408.py\", line 2, in <module>\n",
      "    training_losses, training_rewards = my_agent.train()  File \"/home/harisc/repos/deep-q-learning/src/agents/dqn_agent.py\", line 134, in train\n",
      "    return self.averaged_losses, self.rewards  File \"/home/harisc/repos/deep-q-learning/src/utils/replay_memory.py\", line 27, in add_transition\n",
      "    self.idx += 1  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fccec5125d0>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    return outflag  File \"/tmp/ipykernel_20874/2141352408.py\", line 2, in <module>\n",
      "    training_losses, training_rewards = my_agent.train()  File \"/home/harisc/repos/deep-q-learning/src/agents/dqn_agent.py\", line 134, in train\n",
      "    return self.averaged_losses, self.rewards  File \"/home/harisc/repos/deep-q-learning/src/utils/replay_memory.py\", line 27, in add_transition\n",
      "    self.idx += 1  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fccec5a4fd0>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    return outflag  File \"/tmp/ipykernel_20874/2141352408.py\", line 2, in <module>\n",
      "    training_losses, training_rewards = my_agent.train()  File \"/home/harisc/repos/deep-q-learning/src/agents/dqn_agent.py\", line 134, in train\n",
      "    return self.averaged_losses, self.rewards  File \"/home/harisc/repos/deep-q-learning/src/utils/replay_memory.py\", line 27, in add_transition\n",
      "    self.idx += 1  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fccec5a4fd0>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    return outflag  File \"/tmp/ipykernel_20874/2141352408.py\", line 2, in <module>\n",
      "    training_losses, training_rewards = my_agent.train()  File \"/home/harisc/repos/deep-q-learning/src/agents/dqn_agent.py\", line 134, in train\n",
      "    return self.averaged_losses, self.rewards  File \"/home/harisc/repos/deep-q-learning/src/utils/replay_memory.py\", line 27, in add_transition\n",
      "    self.idx += 1  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fccec5b8250>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    return outflag  File \"/tmp/ipykernel_20874/2141352408.py\", line 2, in <module>\n",
      "    training_losses, training_rewards = my_agent.train()  File \"/home/harisc/repos/deep-q-learning/src/agents/dqn_agent.py\", line 134, in train\n",
      "    return self.averaged_losses, self.rewards  File \"/home/harisc/repos/deep-q-learning/src/utils/replay_memory.py\", line 27, in add_transition\n",
      "    self.idx += 1  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fccec5b8250>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    return outflag  File \"/tmp/ipykernel_20874/2141352408.py\", line 2, in <module>\n",
      "    training_losses, training_rewards = my_agent.train()  File \"/home/harisc/repos/deep-q-learning/src/agents/dqn_agent.py\", line 134, in train\n",
      "    return self.averaged_losses, self.rewards  File \"/home/harisc/repos/deep-q-learning/src/utils/replay_memory.py\", line 27, in add_transition\n",
      "    self.idx += 1  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fccec405fd0>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    return outflag  File \"/tmp/ipykernel_20874/2141352408.py\", line 2, in <module>\n",
      "    training_losses, training_rewards = my_agent.train()  File \"/home/harisc/repos/deep-q-learning/src/agents/dqn_agent.py\", line 134, in train\n",
      "    return self.averaged_losses, self.rewards  File \"/home/harisc/repos/deep-q-learning/src/utils/replay_memory.py\", line 27, in add_transition\n",
      "    self.idx += 1  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fccec405fd0>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    return outflag  File \"/tmp/ipykernel_20874/2141352408.py\", line 2, in <module>\n",
      "    training_losses, training_rewards = my_agent.train()  File \"/home/harisc/repos/deep-q-learning/src/agents/dqn_agent.py\", line 134, in train\n",
      "    return self.averaged_losses, self.rewards  File \"/home/harisc/repos/deep-q-learning/src/utils/replay_memory.py\", line 27, in add_transition\n",
      "    self.idx += 1  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fcce4457750>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    return outflag  File \"/tmp/ipykernel_20874/2141352408.py\", line 2, in <module>\n",
      "    training_losses, training_rewards = my_agent.train()  File \"/home/harisc/repos/deep-q-learning/src/agents/dqn_agent.py\", line 134, in train\n",
      "    return self.averaged_losses, self.rewards  File \"/home/harisc/repos/deep-q-learning/src/utils/replay_memory.py\", line 27, in add_transition\n",
      "    self.idx += 1  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fcce4457750>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    return outflag  File \"/tmp/ipykernel_20874/2141352408.py\", line 2, in <module>\n",
      "    training_losses, training_rewards = my_agent.train()  File \"/home/harisc/repos/deep-q-learning/src/agents/dqn_agent.py\", line 134, in train\n",
      "    return self.averaged_losses, self.rewards  File \"/home/harisc/repos/deep-q-learning/src/utils/replay_memory.py\", line 27, in add_transition\n",
      "    self.idx += 1  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fcce4456750>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    return outflag  File \"/tmp/ipykernel_20874/2141352408.py\", line 2, in <module>\n",
      "    training_losses, training_rewards = my_agent.train()  File \"/home/harisc/repos/deep-q-learning/src/agents/dqn_agent.py\", line 134, in train\n",
      "    return self.averaged_losses, self.rewards  File \"/home/harisc/repos/deep-q-learning/src/utils/replay_memory.py\", line 27, in add_transition\n",
      "    self.idx += 1  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fcce4456750>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    return outflag  File \"/tmp/ipykernel_20874/2141352408.py\", line 2, in <module>\n",
      "    training_losses, training_rewards = my_agent.train()  File \"/home/harisc/repos/deep-q-learning/src/agents/dqn_agent.py\", line 134, in train\n",
      "    return self.averaged_losses, self.rewards  File \"/home/harisc/repos/deep-q-learning/src/utils/replay_memory.py\", line 27, in add_transition\n",
      "    self.idx += 1  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fcce4454210>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    return outflag  File \"/tmp/ipykernel_20874/2141352408.py\", line 2, in <module>\n",
      "    training_losses, training_rewards = my_agent.train()  File \"/home/harisc/repos/deep-q-learning/src/agents/dqn_agent.py\", line 134, in train\n",
      "    return self.averaged_losses, self.rewards  File \"/home/harisc/repos/deep-q-learning/src/utils/replay_memory.py\", line 27, in add_transition\n",
      "    self.idx += 1  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fcce4454210>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    return outflag  File \"/tmp/ipykernel_20874/2141352408.py\", line 2, in <module>\n",
      "    training_losses, training_rewards = my_agent.train()  File \"/home/harisc/repos/deep-q-learning/src/agents/dqn_agent.py\", line 134, in train\n",
      "    return self.averaged_losses, self.rewards  File \"/home/harisc/repos/deep-q-learning/src/utils/replay_memory.py\", line 27, in add_transition\n",
      "    self.idx += 1  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fcce4223390>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    return outflag  File \"/tmp/ipykernel_20874/2141352408.py\", line 2, in <module>\n",
      "    training_losses, training_rewards = my_agent.train()  File \"/home/harisc/repos/deep-q-learning/src/agents/dqn_agent.py\", line 134, in train\n",
      "    return self.averaged_losses, self.rewards  File \"/home/harisc/repos/deep-q-learning/src/utils/replay_memory.py\", line 27, in add_transition\n",
      "    self.idx += 1  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fcce4223390>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    return outflag  File \"/tmp/ipykernel_20874/2141352408.py\", line 2, in <module>\n",
      "    training_losses, training_rewards = my_agent.train()  File \"/home/harisc/repos/deep-q-learning/src/agents/dqn_agent.py\", line 134, in train\n",
      "    return self.averaged_losses, self.rewards  File \"/home/harisc/repos/deep-q-learning/src/utils/replay_memory.py\", line 27, in add_transition\n",
      "    self.idx += 1  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fcce41f2c90>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    return outflag  File \"/tmp/ipykernel_20874/2141352408.py\", line 2, in <module>\n",
      "    training_losses, training_rewards = my_agent.train()  File \"/home/harisc/repos/deep-q-learning/src/agents/dqn_agent.py\", line 134, in train\n",
      "    return self.averaged_losses, self.rewards  File \"/home/harisc/repos/deep-q-learning/src/utils/replay_memory.py\", line 27, in add_transition\n",
      "    self.idx += 1  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fcce41f2c90>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    return outflag  File \"/tmp/ipykernel_20874/2141352408.py\", line 2, in <module>\n",
      "    training_losses, training_rewards = my_agent.train()  File \"/home/harisc/repos/deep-q-learning/src/agents/dqn_agent.py\", line 134, in train\n",
      "    return self.averaged_losses, self.rewards  File \"/home/harisc/repos/deep-q-learning/src/utils/replay_memory.py\", line 27, in add_transition\n",
      "    self.idx += 1  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fccec2a5490>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    return outflag  File \"/tmp/ipykernel_20874/2141352408.py\", line 2, in <module>\n",
      "    training_losses, training_rewards = my_agent.train()  File \"/home/harisc/repos/deep-q-learning/src/agents/dqn_agent.py\", line 134, in train\n",
      "    return self.averaged_losses, self.rewards  File \"/home/harisc/repos/deep-q-learning/src/utils/replay_memory.py\", line 27, in add_transition\n",
      "    self.idx += 1  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fccec2a5490>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    return outflag  File \"/tmp/ipykernel_20874/2141352408.py\", line 2, in <module>\n",
      "    training_losses, training_rewards = my_agent.train()  File \"/home/harisc/repos/deep-q-learning/src/agents/dqn_agent.py\", line 134, in train\n",
      "    return self.averaged_losses, self.rewards  File \"/home/harisc/repos/deep-q-learning/src/utils/replay_memory.py\", line 27, in add_transition\n",
      "    self.idx += 1  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fccb9490610>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    return outflag  File \"/tmp/ipykernel_20874/2141352408.py\", line 2, in <module>\n",
      "    training_losses, training_rewards = my_agent.train()  File \"/home/harisc/repos/deep-q-learning/src/agents/dqn_agent.py\", line 134, in train\n",
      "    return self.averaged_losses, self.rewards  File \"/home/harisc/repos/deep-q-learning/src/utils/replay_memory.py\", line 27, in add_transition\n",
      "    self.idx += 1  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fccb9490610>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    return outflag  File \"/tmp/ipykernel_20874/2141352408.py\", line 2, in <module>\n",
      "    training_losses, training_rewards = my_agent.train()  File \"/home/harisc/repos/deep-q-learning/src/agents/dqn_agent.py\", line 134, in train\n",
      "    return self.averaged_losses, self.rewards  File \"/home/harisc/repos/deep-q-learning/src/utils/replay_memory.py\", line 27, in add_transition\n",
      "    self.idx += 1  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fccb9284210>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    return outflag  File \"/tmp/ipykernel_20874/2141352408.py\", line 2, in <module>\n",
      "    training_losses, training_rewards = my_agent.train()  File \"/home/harisc/repos/deep-q-learning/src/agents/dqn_agent.py\", line 134, in train\n",
      "    return self.averaged_losses, self.rewards  File \"/home/harisc/repos/deep-q-learning/src/utils/replay_memory.py\", line 27, in add_transition\n",
      "    self.idx += 1  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fccb9284210>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    return outflag  File \"/tmp/ipykernel_20874/2141352408.py\", line 2, in <module>\n",
      "    training_losses, training_rewards = my_agent.train()  File \"/home/harisc/repos/deep-q-learning/src/agents/dqn_agent.py\", line 134, in train\n",
      "    return self.averaged_losses, self.rewards  File \"/home/harisc/repos/deep-q-learning/src/utils/replay_memory.py\", line 27, in add_transition\n",
      "    self.idx += 1  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fccb956ff50>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    return outflag  File \"/tmp/ipykernel_20874/2141352408.py\", line 2, in <module>\n",
      "    training_losses, training_rewards = my_agent.train()  File \"/home/harisc/repos/deep-q-learning/src/agents/dqn_agent.py\", line 134, in train\n",
      "    return self.averaged_losses, self.rewards  File \"/home/harisc/repos/deep-q-learning/src/utils/replay_memory.py\", line 27, in add_transition\n",
      "    self.idx += 1  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fccb956ff50>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    return outflag  File \"/tmp/ipykernel_20874/2141352408.py\", line 2, in <module>\n",
      "    training_losses, training_rewards = my_agent.train()  File \"/home/harisc/repos/deep-q-learning/src/agents/dqn_agent.py\", line 134, in train\n",
      "    return self.averaged_losses, self.rewards  File \"/home/harisc/repos/deep-q-learning/src/utils/replay_memory.py\", line 27, in add_transition\n",
      "    self.idx += 1  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fccb9547bd0>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    return outflag  File \"/tmp/ipykernel_20874/2141352408.py\", line 2, in <module>\n",
      "    training_losses, training_rewards = my_agent.train()  File \"/home/harisc/repos/deep-q-learning/src/agents/dqn_agent.py\", line 134, in train\n",
      "    return self.averaged_losses, self.rewards  File \"/home/harisc/repos/deep-q-learning/src/utils/replay_memory.py\", line 27, in add_transition\n",
      "    self.idx += 1  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fccb9547bd0>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
      "    return outflag  File \"/tmp/ipykernel_20874/2141352408.py\", line 2, in <module>\n",
      "    training_losses, training_rewards = my_agent.train()  File \"/home/harisc/repos/deep-q-learning/src/agents/dqn_agent.py\", line 134, in train\n",
      "    return self.averaged_losses, self.rewards  File \"/home/harisc/repos/deep-q-learning/src/utils/replay_memory.py\", line 27, in add_transition\n",
      "    self.idx += 1  File \"/home/harisc/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating target model...\n",
      "Finished 400 frames. Loss: 0.43775338\n",
      "Updating target model...\n",
      "Finished 600 frames. Loss: 0.103982784\n",
      "Episode finished. Reward: 455.0\n",
      "Updating target model...\n",
      "Finished 800 frames. Loss: 0.06901565\n",
      "Updating target model...\n",
      "Finished 1000 frames. Loss: 0.073534444\n",
      "Finished 1200 frames. Loss: 0.06351438\n",
      "Episode finished. Reward: 180.0\n",
      "Updating target model...\n",
      "Finished 1400 frames. Loss: 0.08019242\n",
      "Episode finished. Reward: 55.0\n",
      "Updating target model...\n",
      "Finished 1600 frames. Loss: 0.100744925\n",
      "Updating target model...\n",
      "Finished 1800 frames. Loss: 0.087699145\n",
      "Updating target model...\n",
      "Finished 2000 frames. Loss: 0.14046481\n",
      "Episode finished. Reward: 295.0\n",
      "Finished 2200 frames. Loss: 0.103357404\n",
      "Updating target model...\n",
      "Finished 2400 frames. Loss: 0.18561561\n",
      "Episode finished. Reward: 120.0\n",
      "Updating target model...\n",
      "Finished 2600 frames. Loss: 0.113805264\n",
      "Updating target model...\n",
      "Episode finished. Reward: 50.0\n",
      "Finished 2800 frames. Loss: 0.2014682\n",
      "Updating target model...\n",
      "Finished 3000 frames. Loss: 0.11090807\n",
      "Finished 3200 frames. Loss: 0.13391954\n",
      "Updating target model...\n",
      "Finished 3400 frames. Loss: 0.095879585\n",
      "Episode finished. Reward: 260.0\n",
      "Updating target model...\n",
      "Finished 3600 frames. Loss: 0.12287914\n",
      "Updating target model...\n",
      "Episode finished. Reward: 40.0\n",
      "Finished 3800 frames. Loss: 0.1333764\n",
      "Updating target model...\n",
      "Finished 4000 frames. Loss: 0.1304624\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m training_losses, training_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mmy_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mElapsed time in minutes:\u001b[39m\u001b[38;5;124m\"\u001b[39m, (end\u001b[38;5;241m-\u001b[39mstart)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m)\n",
      "File \u001b[0;32m~/repos/deep-q-learning/src/agents/dqn_agent.py:111\u001b[0m, in \u001b[0;36mSpaceInvaderAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# perform weights update for main model\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame_num \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_main_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m frame_num \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory_warmup:\n\u001b[0;32m--> 111\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# perform weights update for target model\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/deep-q-learning/src/agents/dqn_agent.py:64\u001b[0m, in \u001b[0;36mSpaceInvaderAgent.update_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m terminal_mask \u001b[38;5;241m=\u001b[39m (lives \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Calculate Q values for non-terminal transitions\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m target_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTargetModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m target_q \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscount \u001b[38;5;241m*\u001b[39m target_pred\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# For terminal transitions, use rewards directly\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/deep-q-learning/src/models/cnn_model.py:58\u001b[0m, in \u001b[0;36mCNNModel.best_reward\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbest_reward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_data) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    Function to retrieve the best expected reward given a current state.\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m     model_prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     model_prediction \u001b[38;5;241m=\u001b[39m reduce_max(model_prediction, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_prediction\n",
      "File \u001b[0;32m~/repos/deep-q-learning/dqn/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/repos/deep-q-learning/dqn/lib/python3.11/site-packages/keras/src/engine/training.py:2620\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2611\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m   2612\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2614\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2617\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   2618\u001b[0m         )\n\u001b[0;32m-> 2620\u001b[0m data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2623\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2624\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2628\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2629\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2630\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2631\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2633\u001b[0m \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   2634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m~/repos/deep-q-learning/dqn/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1688\u001b[0m, in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorExactEvalDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1687\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/deep-q-learning/dqn/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1292\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution \u001b[38;5;241m=\u001b[39m steps_per_execution\n\u001b[1;32m   1291\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1292\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpss_evaluation_shards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpss_evaluation_shards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/repos/deep-q-learning/dqn/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:355\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m flat_dataset\n\u001b[1;32m    353\u001b[0m indices_dataset \u001b[38;5;241m=\u001b[39m indices_dataset\u001b[38;5;241m.\u001b[39mflat_map(slice_batch_indices)\n\u001b[0;32m--> 355\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslice_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshuffle_batch\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch):\n",
      "File \u001b[0;32m~/repos/deep-q-learning/dqn/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:396\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.slice_inputs\u001b[0;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrab_batch\u001b[39m(i, data):\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m d: tf\u001b[38;5;241m.\u001b[39mgather(d, i, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), data\n\u001b[1;32m    394\u001b[0m     )\n\u001b[0;32m--> 396\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrab_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAUTOTUNE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;66;03m# Default optimizations are disabled to avoid the overhead of\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# (unnecessary) input pipeline graph serialization and deserialization\u001b[39;00m\n\u001b[1;32m    400\u001b[0m options \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mOptions()\n",
      "File \u001b[0;32m~/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py:2280\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2276\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\u001b[39;00m\n\u001b[1;32m   2277\u001b[0m \u001b[38;5;66;03m# dataset_ops).\u001b[39;00m\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m   2279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m map_op\n\u001b[0;32m-> 2280\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/data/ops/map_op.py:40\u001b[0m, in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _MapDataset(\n\u001b[1;32m     38\u001b[0m       input_dataset, map_func, preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 40\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ParallelMapDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/data/ops/map_op.py:163\u001b[0m, in \u001b[0;36m_ParallelMapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_parallel_calls \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\n\u001b[1;32m    161\u001b[0m     num_parallel_calls, dtype\u001b[38;5;241m=\u001b[39mdtypes\u001b[38;5;241m.\u001b[39mint64, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_parallel_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m--> 163\u001b[0m variant_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_map_dataset_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variant_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_func\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_func\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_deterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_inter_op_parallelism\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_use_inter_op_parallelism\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preserve_cardinality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_common_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(input_dataset, variant_tensor)\n",
      "File \u001b[0;32m~/repos/deep-q-learning/dqn/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py:5797\u001b[0m, in \u001b[0;36mparallel_map_dataset_v2\u001b[0;34m(input_dataset, other_arguments, num_parallel_calls, f, output_types, output_shapes, use_inter_op_parallelism, deterministic, preserve_cardinality, metadata, name)\u001b[0m\n\u001b[1;32m   5795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   5796\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 5797\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5798\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mParallelMapDatasetV2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_arguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5799\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_types\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5800\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_shapes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_inter_op_parallelism\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5801\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_inter_op_parallelism\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeterministic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5802\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreserve_cardinality\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   5804\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "training_losses, training_rewards = my_agent.train()\n",
    "end = time.time()\n",
    "print(\"Elapsed time in minutes:\", (end-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d934c4e-3a97-45b4-bb90-3da2b974c3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time in minutes: 1.1749772548675537\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "evaluation_rewards = my_agent.evaluate()\n",
    "end = time.time()\n",
    "print(\"Elapsed time in minutes:\", (end-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28beab24-efcb-4c39-b433-0ee2c542dbd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dqn",
   "language": "python",
   "name": "dqn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
