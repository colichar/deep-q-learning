{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e25a7b0",
   "metadata": {},
   "source": [
    "# Deep-Q-Learning\n",
    "\n",
    "In this notebook I explore the concepts of Deep-Q-Learning by following the papers published DeepMind [V. Mnih et al (2013)](https://arxiv.org/pdf/1312.5602.pdf) and [V. Mnih et al (2015)](https://www.nature.com/articles/nature14236/).\n",
    "\n",
    "As I'm intrigued by the topic of autonomous driving, the [MIT lecture videos by Lex Fridman](https://deeplearning.mit.edu/) introduced me to the topic of DQL motivated me to take a deep dive and get familiar with its concepts.\n",
    "\n",
    "### !!!This is still a work in progress!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57bac5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Conv2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d504fa",
   "metadata": {},
   "source": [
    "## Preview of the game\n",
    "\n",
    "Use **left** and **right arrows** to move the spaceship sideways and the **space bar** to use the main cannon and shoot the aliens!\n",
    "\n",
    "If you want to move to the left/right and shoot simultaneously then use **s/d**.\n",
    "\n",
    "As you play, the `PlayPlot` function (marked deprecated) will plot the immediate award for the actions you take. For this uncomment the callback call in the `play` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf69d995",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hare/.local/lib/python3.10/site-packages/gym/utils/play.py:322: DeprecationWarning: \u001b[33mWARN: `PlayPlot` is marked as deprecated and will be removed in the near future.\u001b[0m\n",
      "  deprecation(\n",
      "A.L.E: Arcade Learning Environment (version 0.8.0+919230b)\n",
      "[Powered by Stella]\n",
      "\u001b[31mERROR: PlayableGame wrapper works only with rgb_array and rgb_array_list render modes, but your environment render_mode = None.\u001b[0m\n",
      "/home/hare/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:289: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "from gym.utils.play import play, PlayPlot\n",
    "\n",
    "def compute_metrics(obs_t, obs_tp, action, reward, terminated, truncated, info):\n",
    "    return [reward, np.linalg.norm(action)]\n",
    "\n",
    "plotter = PlayPlot(\n",
    "    compute_metrics,\n",
    "    horizon_timesteps=200,\n",
    "    plot_names=[\"Immediate Rew.\", \"Action Magnitude\"]\n",
    ")\n",
    "\n",
    "my_env = gym.make(\"SpaceInvaders-v4\", render_mode=\"rgb_array\")\n",
    "mapping = {(pygame.K_SPACE,): 1, (pygame.K_RIGHT,): 2, (pygame.K_LEFT,): 3, (pygame.K_d,): 4, (pygame.K_s,): 5}\n",
    "play(my_env, keys_to_action=mapping) #, callback=plotter.callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bdb7e9",
   "metadata": {},
   "source": [
    "## Some random action taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08668fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.0+919230b)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"SpaceInvaders-v4\", render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "    action = choose_action(env, observation, 1)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f96bcd0",
   "metadata": {},
   "source": [
    "## Starting to define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef629b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action (env, obs, eps):\n",
    "    # get a random number\n",
    "    random_number = np.random.rand()\n",
    "    \n",
    "    if random_number < eps:\n",
    "        # we explore\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "    else:\n",
    "        # we choose the action yielding the highest reward according to our model\n",
    "        action = env.action_space.sample()\n",
    "    \n",
    "    return action\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791e4699",
   "metadata": {},
   "source": [
    "For the first attempt we'll use the architecture of the neural network described on page 6 in [Volodymir Mnih et al (2013)](https://arxiv.org/pdf/1312.5602.pdf).\n",
    "\n",
    "### Preprocess function $\\phi$\n",
    "\n",
    "Since it will be computationally more demanding working with the raw $210 \\times 160$ pixel frames, we'll apply a preprocess function $\\phi$ to the frames before forwarding them to the neural network. The preprocess function will do the following\n",
    "- for a single frame we take the maximum value for each pixel color value and the previous frame (removes flickering)\n",
    "- the frames will be converted from RGB representation into gray-scale\n",
    "- the frames will be resized to $110 \\times 84$ images\n",
    "- the playing area is cropped out so we have a final $84 \\times 84$ frame (this is done in, but I might skip this step in this notebook [Volodymir Mnih et al (2013)](https://arxiv.org/pdf/1312.5602.pdf))\n",
    "- the last 4 frames from a sequence will be modified as explained and stacked to form a $84 \\times 84 \\times 4$ input for the agent\n",
    "\n",
    "So, to preprocess our input for the neuarl network, we'll need a function $\\phi$ that will take the previous processed sequence $\\phi_t$ and new obtained frame $x_{t+1}$ and combine them to a new processed sequence as follows\n",
    "\n",
    "$$\\phi: \\phi_t, x_{t+1} \\longrightarrow \\phi_{t+1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cfa2ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get some observations to test the preprocess function\n",
    "\n",
    "memory = []\n",
    "\n",
    "env = gym.make(\"SpaceInvaders-v4\", render_mode=\"rgb_array\")\n",
    "observation, info = env.reset(seed=42)\n",
    "preprocessed = preprocess_input(observation)\n",
    "memory.append(preprocessed)\n",
    "for _ in range(300):\n",
    "    action = choose_action(env, observation, 1)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    preprocessed = preprocess_input([memory[-1], observation])\n",
    "    \n",
    "    memory.append(preprocessed)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ad9e76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_state (env, curr_obs):\n",
    "    \"\"\"\n",
    "    Initializes the first state of an episode with the first 4 frames.\n",
    "    Args:\n",
    "        env: Atari games gym environment,\n",
    "        cur_obs: first/current observation gained from reseting the environment\n",
    "    Returns:\n",
    "        a 84x84x4 input for our NN consisting of 4 preprocessed frames\n",
    "    \"\"\"\n",
    "    curr_obs = preprocess_input(curr_obs)\n",
    "    processed = [curr_obs]\n",
    "    \n",
    "    for i in range(3):\n",
    "        curr_action = choose_action(env, curr_obs, eps)\n",
    "        curr_obs = my_env.step(curr_action)[0]\n",
    "        curr_obs = preprocess_input([processed[-1], curr_obs]) \n",
    "        processed.append(curr_obs)\n",
    "        \n",
    "    return processed[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a408b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input (state):\n",
    "        # define a preprocess function for the frames recieving from the Atari games\n",
    "        # we'll follow again the preprocess function proposed by Volodymir Mnih et al (2013)\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state: s_t+1 consisting of s_t, a_t, x_t+1\n",
    "        Returns:\n",
    "            transition: consists of phi_t, a_t, r_t, phi_t+1\n",
    "        \"\"\"\n",
    "        \n",
    "        if type(state) != type(np.zeros(0)):\n",
    "            prev_seq, new_frame = state\n",
    "\n",
    "            ## drop out the oldest frame\n",
    "            if prev_seq.shape[2] > 3:\n",
    "                prev_seq = prev_seq[::, ::, 1:]\n",
    "\n",
    "            ## process new frame\n",
    "\n",
    "            processed_fr = tf.image.rgb_to_grayscale(new_frame)\n",
    "            processed_fr = tf.image.crop_to_bounding_box(processed_fr, 34, 0, 160, 160)\n",
    "            processed_fr = tf.image.resize(processed_fr, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            \n",
    "            processed_state = tf.concat([prev_seq, processed_fr], -1)\n",
    "\n",
    "\n",
    "            return processed_state\n",
    "        \n",
    "        else:\n",
    "            new_frame = state\n",
    "            \n",
    "            ## process new frame\n",
    "        \n",
    "            processed_fr = tf.image.rgb_to_grayscale(new_frame)\n",
    "            processed_fr = tf.image.crop_to_bounding_box(processed_fr, 34, 0, 160, 160)\n",
    "            processed_fr = tf.image.resize(processed_fr, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            \n",
    "            return tf.expand_dims(processed_fr[::, ::, -1],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccc0c400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent (obs_shape, action_shape, learning_rate):\n",
    "    # define a Sequential CNN using Keras predefined layers\n",
    "    # Here we'll use the same architecture as in Volodymir Mnih et al (2013)\n",
    "    model = keras.Sequential()\n",
    "    model.add(Conv2D(filters=16, kernel_size=(8, 8), strides=(4, 4), padding=\"same\", activation=\"relu\", input_shape=(84, 84, 4)))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(4, 4), strides=(2, 2), padding=\"same\", activation=\"relu\", input_shape=(84, 84, 4)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dense(action_shape, activation=\"relu\"))\n",
    "\n",
    "    model.compile(loss=keras.losses.Huber(), optimizer=keras.optimizers.Adam(learning_rate=learning_rate), metrics=['accuracy'])        \n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b02785e",
   "metadata": {},
   "source": [
    "### Replay memory\n",
    "\n",
    "The replay memory D stores following information (called *transition*) every step\n",
    "\n",
    "$$\\left( \\phi_t, \\ a_t, \\ r_t, \\ \\phi_{t+1} \\right) \\longrightarrow D$$\n",
    "\n",
    "where\n",
    "- $\\phi_t$ is the preprocessed sequence including the frame at time step $t$\n",
    "- $a_t$ is the action taken at time step $t$\n",
    "- $r_t$ is the reward gained at time step $t$ after taking action $a_t$ in state $\\phi_t$ $t$\n",
    "- $\\phi_{t+1}$ is the preprocessed sequence including the new state we reached after taking action $a_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6522526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class replay_memory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memories = []\n",
    "        \n",
    "    def store_transition(self, curr_state, curr_action, reward, new_state):\n",
    "        # store transition in replay_memory\n",
    "        self.memories.append([curr_state, curr_action, reward, new_state])\n",
    "        if (len(self.memories) > self.capacity):\n",
    "            self.memories.pop(0)\n",
    "    \n",
    "    def sample_minibatch(self):\n",
    "        # sample a minibatch of transition from replay_memory\n",
    "        return 0\n",
    "    \n",
    "    def get_memories(self):\n",
    "        return self.memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3213a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_env = gym.make(\"SpaceInvaders-v4\",render_mode=\"rgb_array\")\n",
    "obs_shape = my_env.observation_space.shape\n",
    "action_shape = my_env.action_space.n\n",
    "\n",
    "memory_size = 1000\n",
    "num_of_episodes = 20\n",
    "minibatch_size = 32\n",
    "update_target_freq = 5    ## update target NN every 5 parameter updates (steps)\n",
    "discount = 0.99\n",
    "action_repeat = 4         ## repeat chosen action 4 times\n",
    "update_main_freq = 4      ## SGD update main NN after 4 chosen actions\n",
    "learning_rate = 0.001\n",
    "grad_mom = 0.95\n",
    "sq_grad_mom = 0.95\n",
    "min_sq_grad = 0.01\n",
    "eps = 1\n",
    "\n",
    "warm_up = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a499e311",
   "metadata": {},
   "source": [
    "## Deep-Q-learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d8e06640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DQL():\n",
    "    ## initialize replay memory D\n",
    "    memories = replay_memory(memory_size)\n",
    "    \n",
    "    ## initialize action-value function Q with random weights\n",
    "    main_model = agent(obs_shape, action_shape, learning_rate)\n",
    "    \n",
    "    ## initialize action-value target function with same weights \n",
    "    target_model = agent(obs_shape, action_shape, learning_rate)\n",
    "    target_model.set_weights(main_model.get_weights())\n",
    "    \n",
    "    memory_count = 0\n",
    "    \n",
    "    ## start outer loop of the number of episode we'll train the model for\n",
    "    for episode in range(num_of_episodes):\n",
    "\n",
    "        ## initialise sequence s1 = {x1} and preprocessed the sequence\n",
    "        curr_obs, info = my_env.reset()\n",
    "        curr_state = initialize_state(my_env, curr_obs)\n",
    "        \n",
    "        while (info[\"lives\"] > 0):\n",
    "            \n",
    "            ## choose an exploration/explotation action \n",
    "            curr_action = choose_action(my_env, curr_state, eps)\n",
    "            \n",
    "            for i in range(4): ## repeat an action 4 times\n",
    "                ## take action\n",
    "                new_obs, rew, terminated, truncated, info = my_env.step(curr_action)\n",
    "\n",
    "                ## create new sequence with new frame\n",
    "                new_state = preprocess_input([curr_state, new_obs])\n",
    "\n",
    "                ## store new transition\n",
    "                memories.store_transition(curr_state, curr_action, rew, new_state)\n",
    "                memory_count +=1\n",
    "                \n",
    "                curr_state = new_state\n",
    "                \n",
    "        print(memory_count)\n",
    "        return memories.get_memories()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1f9bd7",
   "metadata": {},
   "source": [
    "<img src=\"./images/DQL-algorithm-2015.png\" alt=\"dql\" width=\"850\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5ce807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
