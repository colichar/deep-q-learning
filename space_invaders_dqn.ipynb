{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e25a7b0",
   "metadata": {},
   "source": [
    "# Deep-Q-Learning\n",
    "\n",
    "In this notebook I explore the concepts of Deep-Q-Learning by following the paper of DeepMind [Volodymir Mnih et al (2013)](https://arxiv.org/pdf/1312.5602.pdf) which was published back in 2013.\n",
    "\n",
    "As I'm intrigued by the topic of autonomous driving, the [MIT lecture videos by Lex Fridman](https://deeplearning.mit.edu/) introduced me to the topic of DQL motivated me to take a deep dive and get familiar with its concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57bac5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Conv2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d504fa",
   "metadata": {},
   "source": [
    "## Preview of the game\n",
    "\n",
    "Use **left** and **right arrows** to move the spaceship sideways and the **space bar** to use the main cannon and shoot the aliens!\n",
    "\n",
    "If you want to move to the left/right and shoot simultaneously then use **s/d**.\n",
    "\n",
    "As you play, the `PlayPlot` function (marked deprecated) will plot the immediate award for the actions you take. For this uncomment the callback call in the `play` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf69d995",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hare/.local/lib/python3.10/site-packages/gym/utils/play.py:322: DeprecationWarning: \u001b[33mWARN: `PlayPlot` is marked as deprecated and will be removed in the near future.\u001b[0m\n",
      "  deprecation(\n",
      "A.L.E: Arcade Learning Environment (version 0.8.0+919230b)\n",
      "[Powered by Stella]\n",
      "\u001b[31mERROR: PlayableGame wrapper works only with rgb_array and rgb_array_list render modes, but your environment render_mode = None.\u001b[0m\n",
      "/home/hare/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:289: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "from gym.utils.play import play, PlayPlot\n",
    "\n",
    "def compute_metrics(obs_t, obs_tp, action, reward, terminated, truncated, info):\n",
    "    return [reward, np.linalg.norm(action)]\n",
    "\n",
    "plotter = PlayPlot(\n",
    "    compute_metrics,\n",
    "    horizon_timesteps=200,\n",
    "    plot_names=[\"Immediate Rew.\", \"Action Magnitude\"]\n",
    ")\n",
    "\n",
    "my_env = gym.make(\"SpaceInvaders-v4\", render_mode=\"rgb_array\")\n",
    "mapping = {(pygame.K_SPACE,): 1, (pygame.K_RIGHT,): 2, (pygame.K_LEFT,): 3, (pygame.K_d,): 4, (pygame.K_s,): 5}\n",
    "play(my_env, keys_to_action=mapping) #, callback=plotter.callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bdb7e9",
   "metadata": {},
   "source": [
    "## Some random action taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08668fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"SpaceInvaders-v4\", render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "    action = choose_action(env, observation, 0.3)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f96bcd0",
   "metadata": {},
   "source": [
    "## Starting to define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef629b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action (env, obs, eps):\n",
    "    # get a random number\n",
    "    random_number = np.random.rand()\n",
    "    \n",
    "    if random_number < eps:\n",
    "        # we explore\n",
    "        action = env.action_space.sample()\n",
    "        print('we explore!')\n",
    "        \n",
    "    else:\n",
    "        # we choose the action yielding the highest reward according to our model\n",
    "        action = env.action_space.sample()\n",
    "        print('we choose the best action!')\n",
    "    \n",
    "    return action\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791e4699",
   "metadata": {},
   "source": [
    "For the first attempt we'll use the architecture of the neural network described on page 6 in [Volodymir Mnih et al (2013)](https://arxiv.org/pdf/1312.5602.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ccc0c400",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_network:\n",
    "\n",
    "    def __init__ (self, obs_shape, action_shape, learning_rate):\n",
    "        # define a Sequential CNN using Keras predefined layers\n",
    "        # Here we'll use the same architecture as in Volodymir Mnih et al (2013)\n",
    "        self.model = keras.Sequential()\n",
    "        self.model.add(Conv2D(filters=16, kernel_size=(8, 8), strides=(4, 4), padding=\"same\", activation=\"relu\", input_shape=(84, 84, 4)))\n",
    "        self.model.add(Conv2D(filters=32, kernel_size=(4, 4), strides=(2, 2), padding=\"same\", activation=\"relu\", input_shape=(84, 84, 4)))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(256, activation=\"relu\"))\n",
    "        self.model.add(Dense(action_shape, activation=\"relu\"))\n",
    "\n",
    "        self.model.compile(loss=keras.losses.Huber(), optimizer=keras.optimizers.Adam(learning_rate=learning_rate), metrics=['accuracy'])\n",
    "        \n",
    "    def preprocess_input (self, obs_shape, frames):\n",
    "        # define a preprocess function for the frames recieving from the Atari games\n",
    "        # we'll follow again the preprocess function proposed by Volodymir Mnih et al (2013)\n",
    "        \n",
    "        # we take the last 4 frames from the sequence\n",
    "        # will it be a simple list or something else?\n",
    "        frames = frames[-4:]\n",
    "        # first we convert the RGB representation to gray-scale\n",
    "        preprocessed_frames = tf.image.rgb_to_grayscale(frames)\n",
    "        # then we size the frames down from 210x160 to 110x84\n",
    "        preprocessed_frames = tf.image.resize(preprocessed_frames, [110,84])\n",
    "        # finally we crop out the 84x84 region that captures roughly the playing area\n",
    "        # will try to add it later, they did it because they were using GPU 2D CNs which expect square inputs\n",
    "        \n",
    "        sequence = np.concatenate((preprocessed_frames), axis=2) # stack the four frames next to each other to get 110x84x4\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6522526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class replay_memory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memories = []\n",
    "        \n",
    "    def store_transition(self, transition):\n",
    "        # store transition in replay_memory\n",
    "        return 0\n",
    "    \n",
    "    def sample_minibatch(self):\n",
    "        # sample a minibatch of transition from replay_memory\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3213a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "my_env = gym.make(\"SpaceInvaders-v4\", render_mode=\"rgb_array\")\n",
    "obs_shape = my_env.observation_space.shape\n",
    "action_shape = my_env.action_space.n\n",
    "\n",
    "num_of_episodes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8e06640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DQL():\n",
    "    ## initialize replay memory D\n",
    "    memories = replay_memory(N)\n",
    "    \n",
    "    ## initialize action-value function Q with random weights\n",
    "    ## The action-value function in DQL is a neural network, so we'll initiate a model\n",
    "    main_model = neural_network(obs_shape, action_shape, learning_rate)\n",
    "    \n",
    "    ## I may need a target model to save the \"older\" weights of the model, was not mentioned in paper\n",
    "    ## target_model = neural_network(obs_shape, action_shape, learning_rate)\n",
    "    \n",
    "    ## start outer loop of the number of episode we'll train the model for\n",
    "    for episode in range(num_of_episodes):\n",
    "        print('helloo')\n",
    "        observation, info = my_env.reset()\n",
    "        \n",
    "        ## initialise sequence s1 = {x1} and preprocessed the sequence\n",
    "        \n",
    "        while (info[lives] > 0):\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7ceaa8da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 0,  0,  0],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0],\n",
       "         ...,\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0]],\n",
       " \n",
       "        [[ 0,  0,  0],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0],\n",
       "         ...,\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0]],\n",
       " \n",
       "        [[ 0,  0,  0],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0],\n",
       "         ...,\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[80, 89, 22],\n",
       "         [80, 89, 22],\n",
       "         [80, 89, 22],\n",
       "         ...,\n",
       "         [80, 89, 22],\n",
       "         [80, 89, 22],\n",
       "         [80, 89, 22]],\n",
       " \n",
       "        [[80, 89, 22],\n",
       "         [80, 89, 22],\n",
       "         [80, 89, 22],\n",
       "         ...,\n",
       "         [80, 89, 22],\n",
       "         [80, 89, 22],\n",
       "         [80, 89, 22]],\n",
       " \n",
       "        [[80, 89, 22],\n",
       "         [80, 89, 22],\n",
       "         [80, 89, 22],\n",
       "         ...,\n",
       "         [80, 89, 22],\n",
       "         [80, 89, 22],\n",
       "         [80, 89, 22]]], dtype=uint8),\n",
       " 3)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation, info = my_env.reset()\n",
    "observation, info[\"lives\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ba2b24",
   "metadata": {},
   "source": [
    "## Deep-Q-learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1f9bd7",
   "metadata": {},
   "source": [
    "<img src=\"./DQL-algorithm.png\" alt=\"dql\" width=\"850\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
